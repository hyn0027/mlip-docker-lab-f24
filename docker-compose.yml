version: '3.9'

services:
  training:
    #Build the container using Dockerfile.train
    build:
      context: .
      dockerfile: Dockerfile.train
    # Use a shared volume to store the trained model
    volumes:
      - model_storage:/app/models
    # DONE: Set the command to run the training script
    command: ["python", "train.py"]
    restart: on-failure

  inference:
    # DONE: Build the container using Dockerfile.infer
    build:
      context: .
      dockerfile: Dockerfile.infer 
    #Use a shared volume to load the trained model
    volumes:
      - model_storage:/app/models
    # DONE: Expose port 8080 (or any other port) for the Flask app
    ports:
      - "8080:8080"
    # Ensure that the inference service starts after the training service is complete
    depends_on:
      training:
        condition: service_completed_successfully
    command: ["python", "server.py"]

#Define a shared volume for the model file
volumes:
  model_storage:
    driver: local
